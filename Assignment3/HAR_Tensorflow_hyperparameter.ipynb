{"cells":[{"cell_type":"code","source":["# All Includes\n\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\nfrom sklearn import metrics\n\nimport os"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport sys\n\nimport tensorflow.python.platform\n\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport itertools"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["NUM_LABELS = 6\nVALIDATION_SIZE = 500  # Size of the validation set.\nSEED = 66478  # Set to None for random seed.\nBATCH_SIZE = 64\nNUM_EPOCHS = 2"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Useful Constants\n\n# Those are separate normalised input features for the neural network\nINPUT_SIGNAL_TYPES = [\n    \"body_acc_x_\",\n    \"body_acc_y_\",\n    \"body_acc_z_\",\n    \"body_gyro_x_\",\n    \"body_gyro_y_\",\n    \"body_gyro_z_\",\n    \"total_acc_x_\",\n    \"total_acc_y_\",\n    \"total_acc_z_\"\n]\n\n# Output classes to learn how to classify\nLABELS = [\n    \"WALKING\", \n    \"WALKING_UPSTAIRS\", \n    \"WALKING_DOWNSTAIRS\", \n    \"SITTING\", \n    \"STANDING\", \n    \"LAYING\"\n]"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["DATASET_PATH = \"/FileStore/tables/\"\nTRAIN = \"train/\"\nTEST = \"test/\"\n\n\n# Load \"X\" (the neural network's training and testing inputs)\n\ndef load_X(X_signals_paths):\n    X_signals = []\n    \n    for signal_type_path in X_signals_paths:\n      #with open(signal_type_path, \"r\") as file:\n        file = open(\"/dbfs/\"+signal_type_path, 'r')\n        # Read dataset from disk, dealing with text files' syntax\n        X_signals.append(\n            [np.array(serie, dtype=np.float32) for serie in [\n                row.replace('  ', ' ').strip().split(' ') for row in file\n            ]]\n        )\n        file.close()\n    \n    return np.transpose(np.array(X_signals), (1, 2, 0))\n\nX_train_signals_paths = [\n    DATASET_PATH + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n]\nX_test_signals_paths = [\n    DATASET_PATH + TEST +  signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n]\n\nX_train = load_X(X_train_signals_paths)\nX_test = load_X(X_test_signals_paths)\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Load \"y\" (the neural network's training and testing outputs)\n\ndef load_y(y_path):\n    file = open(\"/dbfs\"+y_path, 'r')\n    # Read dataset from disk, dealing with text file's syntax\n    y_ = np.array(\n        [elem for elem in [\n            row.replace('  ', ' ').strip().split(' ') for row in file\n        ]], \n        dtype=np.int32\n    )\n    file.close()\n    \n    # Substract 1 to each output class for friendly 0-based indexing \n    return y_ - 1\n\ny_train_path = DATASET_PATH + \"y_train.txt\"\ny_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n\ny_train = load_y(y_train_path)\ny_test = load_y(y_test_path)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Input Data \n\ntraining_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\ntest_data_count = len(X_test)  # 2947 testing series\nn_steps = len(X_train[0])  # 128 timesteps per series\nn_input = len(X_train[0][0])  # 9 input parameters per timestep"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# LSTM Neural Network's internal structure\n\nn_hidden = 32 # Hidden layer num of features\nn_classes = 6 # Total classes (should go up, or should go down)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Training \n\nlearning_rate = 0.0025\nlambda_loss_amount = 0.0015\ntraining_iters = training_data_count * 300  # Loop 300 times on the dataset\nbatch_size = 1500\ndisplay_iter = 30000  # To show test set accuracy during training"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Some debugging info\n\nprint(\"Some useful info to get an insight on dataset's shape and normalisation:\")\nprint(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\nprint(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\nprint(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["def LSTM_RNN(_X, _weights, _biases):\n    # Function returns a tensorflow LSTM (RNN) artificial neural network from given parameters. \n    # Moreover, two LSTM cells are stacked which adds deepness to the neural network. \n    # Note, some code of this notebook is inspired from an slightly different \n    # RNN architecture used on another dataset, some of the credits goes to \n    # \"aymericdamien\" under the MIT license.\n\n    # (NOTE: This step could be greatly optimised by shaping the dataset once\n    # input shape: (batch_size, n_steps, n_input)\n    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n    # Reshape to prepare input to hidden activation\n    _X = tf.reshape(_X, [-1, n_input]) \n    # new shape: (n_steps*batch_size, n_input)\n    \n    # Linear activation\n    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n    _X = tf.split(_X, n_steps, 0) \n    # new shape: n_steps * (batch_size, n_hidden)\n\n    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n    # Get LSTM cell output\n    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n\n    # Get last time step's output feature for a \"many to one\" style classifier, \n    # as in the image describing RNNs at the top of this page\n    lstm_last_output = outputs[-1]\n    \n    # Linear activation\n    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["def extract_batch_size(_train, step, batch_size):\n    # Function to fetch a \"batch_size\" amount of data from \"(X|y)_train\" data. \n    \n    shape = list(_train.shape)\n    shape[0] = batch_size\n    batch_s = np.empty(shape)\n\n    for i in range(batch_size):\n        # Loop index\n        index = ((step-1)*batch_size + i) % len(_train)\n        batch_s[i] = _train[index] \n\n    return batch_s"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["def one_hot(y_):\n    # Function to encode output labels from number indexes \n    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n    \n    y_ = y_.reshape(len(y_))\n    n_values = int(np.max(y_)) + 1\n    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Graph input/output\nx = tf.placeholder(tf.float32, [None, n_steps, n_input])\ny = tf.placeholder(tf.float32, [None, n_classes])\n\n# Graph weights\nweights = {\n    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n}\nbiases = {\n    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["pred = LSTM_RNN(x, weights, biases)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Loss, optimizer and evaluation\nl2 = lambda_loss_amount * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n# L2 loss prevents this overkill neural network to overfit the data\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n\ncorrect_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["base_learning_rates = [0.00001,0.025,0.0025,0.00001]\nbase_batch_size = [1500, 1000, 2000]\nall_experiments = list(itertools.product(base_learning_rates, base_batch_size))\nprint(len(all_experiments))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["all_exps_rdd = sc.parallelize(all_experiments, numSlices=len(all_experiments))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["def run_model(learning_rates,batch_sizes):\n  test_losses = []\n  test_accuracies = []\n  train_losses = []\n  train_accuracies = []\n  \n  res = {}\n  res['base_learning_rate'] = base_learning_rate\n  res['minibatch_loss'] = 100.0\n  res['test_error'] = 100.0\n  res['validation_error'] = 100.0\n  \n    # Launch the graph\n  sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n  init = tf.global_variables_initializer()\n  sess.run(init)\n\n  # Perform Training steps with \"batch_size\" amount of example data at each loop\n  step = 1\n  while step * batch_size <= training_iters:\n      batch_xs =         extract_batch_size(X_train, step, batch_sizes)\n      batch_ys = one_hot(extract_batch_size(y_train, step, batch_sizes))\n\n      # Fit training using batch data\n      _, loss, acc = sess.run(\n          [optimizer, cost, accuracy],\n          feed_dict={\n              x: batch_xs, \n              y: batch_ys\n          }\n      )\n      train_losses.append(loss)\n      train_accuracies.append(acc)\n\n      # Evaluate network only at some steps for faster training: \n      if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n\n          # To not spam console, show training accuracy/loss in this \"if\"\n          print(\"Training iter #\" + str(step*batch_size) + \\\n                \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n                \", Accuracy = {}\".format(acc))\n\n          # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n          loss, acc = sess.run(\n              [cost, accuracy], \n              feed_dict={\n                  x: X_test,\n                  y: one_hot(y_test)\n              }\n          )\n          test_losses.append(loss)\n          test_accuracies.append(acc)\n          print(\"PERFORMANCE ON TEST SET: \" + \\\n                \"Batch Loss = {}\".format(loss) + \\\n                \", Accuracy = {}\".format(acc))\n\n      step += 1\n  res['test_error'] = loss\n  res['test_acc'] = acc\n  print(\"Optimization Finished!\")\n  return res\n  "],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["num_nodes = 4\nn = max(2, int(len(all_experiments) // num_nodes))\ngrouped_experiments = [all_experiments[i:i+n] for i in range(0, len(all_experiments), n)]\nall_exps_rdd = sc.parallelize(grouped_experiments, numSlices=len(grouped_experiments))\nresults = all_exps_rdd.flatMap(lambda z: [run_model(*y) for y in z]).collect()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# Accuracy for test data\n\none_hot_predictions, accuracy, final_loss = sess.run(\n    [pred, accuracy, cost],\n    feed_dict={\n        x: X_test,\n        y: one_hot(y_test)\n    }\n)\n\ntest_losses.append(final_loss)\ntest_accuracies.append(accuracy)\n\nprint(\"FINAL RESULT: \" + \\\n      \"Batch Loss = {}\".format(final_loss) + \\\n      \", Accuracy = {}\".format(accuracy))"],"metadata":{},"outputs":[],"execution_count":21}],"metadata":{"name":"HAR_Tensorflow_hyperparameter","notebookId":3300760951503223},"nbformat":4,"nbformat_minor":0}
