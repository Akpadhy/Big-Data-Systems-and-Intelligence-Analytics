{"cells":[{"cell_type":"code","source":["##Part B - Structured Streaming (30 Points)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["\ndisplay(dbutils.fs.ls(\"/FileStore/tables\"))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.types import *"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["inputPath = \"dbfs:/FileStore/tables/\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["string = \"_c\"\ncolumn_name = [string + str(i) for i in range(0,129,1)]"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["fields = [StructField( x , StringType(), True) for x in column_name] "],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["  csvSchema= StructType(fields) #creating schema for the input files"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["staticInputDF = (\n  spark\n  #  .read\n    .readStream\n    .schema(csvSchema)\n    .option(\"maxFilesPerTrigger\", 1)\n    .csv(inputPath, header=True)\n)\n\ndisplay(staticInputDF)\nstaticInputDF.isStreaming\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from pyspark.sql.functions import *\n#selecting 50% of data as the readings are taken at 50% overlap (128 readings/window). \nstreamingCountsDF = ( \n       staticInputDF\n       .select(staticInputDF.columns[:64])\n       .drop(\"_c0\")\n       \n       )\n\nstreamingCountsDF.createOrReplaceTempView(\"Grouped_data\")\ndisplay(streamingCountsDF)\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"console\")        # memory = store in-memory table (for testing only in Spark 2.0)\n    .queryName(\"counts\")     # counts = name of the in-memory table\n    #.outputMode(\"\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from time import sleep\nsleep(5)  # wait"],"metadata":{},"outputs":[],"execution_count":12}],"metadata":{"name":"StreamingHRA","notebookId":3557088432949929},"nbformat":4,"nbformat_minor":0}
